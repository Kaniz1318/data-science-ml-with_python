{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "80dd7ba8",
      "metadata": {
        "id": "80dd7ba8"
      },
      "source": [
        "# 🐼 Complete Pandas Tutorial for Beginners\n",
        "\n",
        "Welcome to the most comprehensive pandas tutorial for beginners! 🎉\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "This notebook will take you from zero to hero in pandas - Python's most powerful data analysis library. Think of pandas as **\"Excel for Python\"** - but much more powerful!\n",
        "\n",
        "### 🎯 Core Philosophy\n",
        "- **Analogy is King:** We'll relate everything to Excel concepts you already know\n",
        "- **Practical Examples:** Using the famous Titanic dataset - small, intuitive, and fun!\n",
        "- **Learn by Doing:** Every concept comes with hands-on examples\n",
        "\n",
        "### 📋 What We'll Cover\n",
        "1. **The Basics:** Series, DataFrames, and why pandas rocks\n",
        "2. **Data Loading:** Getting your data into pandas\n",
        "3. **Data Inspection:** The \"holy trinity\" of data exploration\n",
        "4. **Selecting & Filtering:** The real magic begins here\n",
        "5. **Data Cleaning:** Handling messy real-world data\n",
        "6. **Grouping & Aggregation:** The power of the \"Split-Apply-Combine\" pattern\n",
        "7. **Combining Data:** Merging and concatenating DataFrames\n",
        "8. **Mini-Project:** Putting it all together!\n",
        "\n",
        "### 🚀 Let's Get Started!\n",
        "\n",
        "**Remember:** Pandas is like Excel, but:\n",
        "- A DataFrame = A spreadsheet\n",
        "- A Series = A single column\n",
        "- Filtering = Excel's filter dropdowns\n",
        "- GroupBy = Excel's PivotTables\n",
        "- But with the power of programming! 💪"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d6154d8",
      "metadata": {
        "id": "0d6154d8"
      },
      "source": [
        "## 📦 Section 1: Import Libraries and Load Dataset\n",
        "\n",
        "First things first - let's import the libraries we need and load our dataset. We'll use the famous Titanic dataset because it's:\n",
        "- Small and manageable\n",
        "- Has interesting categorical and numerical data\n",
        "- Tells a story everyone can relate to\n",
        "- Perfect for learning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e2c11a3",
      "metadata": {
        "id": "3e2c11a3"
      },
      "outputs": [],
      "source": [
        "# Import the essential libraries\n",
        "import pandas as pd  # The 'pd' is a universal convention - everyone uses this!\n",
        "import numpy as np   # For numerical operations\n",
        "\n",
        "# Let's check our pandas version\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(\"🎉 Ready to explore data with pandas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23971470",
      "metadata": {
        "id": "23971470"
      },
      "outputs": [],
      "source": [
        "# Let's create our dataset first - a simplified Titanic dataset\n",
        "# In real life, you'd use pd.read_csv('titanic.csv')\n",
        "\n",
        "# Creating sample data that represents the Titanic dataset\n",
        "titanic_data = {\n",
        "    'PassengerId': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
        "    'Survived': [0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0],\n",
        "    'Pclass': [3, 1, 3, 1, 3, 3, 1, 3, 2, 3, 1, 3],\n",
        "    'Name': ['Braund, Mr. Owen Harris', 'Cumings, Mrs. John Bradley',\n",
        "             'Heikkinen, Miss. Laina', 'Futrelle, Mrs. Jacques Heath',\n",
        "             'Allen, Mr. William Henry', 'Moran, Mr. James',\n",
        "             'McCarthy, Mr. Timothy J', 'Palsson, Master. Gosta Leonard',\n",
        "             'Johnson, Mrs. Oscar W', 'Nasser, Mrs. Nicholas',\n",
        "             'Sandstrom, Miss. Marguerite Rut', 'Bonnell, Miss. Elizabeth'],\n",
        "    'Sex': ['male', 'female', 'female', 'female', 'male', 'male',\n",
        "            'male', 'male', 'female', 'female', 'female', 'female'],\n",
        "    'Age': [22.0, 38.0, 26.0, 35.0, 35.0, None, 54.0, 2.0, 27.0, 14.0, 4.0, 58.0],\n",
        "    'Fare': [7.25, 71.28, 7.92, 53.1, 8.05, 8.46, 51.86, 21.08, 11.13, 30.07, 16.7, 26.55]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame - think of this as opening a CSV file in Excel!\n",
        "df = pd.DataFrame(titanic_data)\n",
        "\n",
        "print(\"✅ Dataset created successfully!\")\n",
        "print(f\"📊 Shape: {df.shape} (that means {df.shape[0]} rows and {df.shape[1]} columns)\")\n",
        "print(\"\\n🎯 Think of this as your Excel spreadsheet, but in Python!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5513af75",
      "metadata": {
        "id": "5513af75"
      },
      "source": [
        "## 🏗️ Section 2: First Look at DataFrames and Series\n",
        "\n",
        "Before we dive into our Titanic data, let's understand the two core building blocks of pandas:\n",
        "\n",
        "- **Series**: A 1D array (like a single column in Excel)\n",
        "- **DataFrame**: A 2D table (like an entire Excel spreadsheet)\n",
        "\n",
        "### 🎯 The Excel Analogy:\n",
        "- Series = One column in Excel\n",
        "- DataFrame = The entire spreadsheet with multiple columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04cb6ad0",
      "metadata": {
        "id": "04cb6ad0"
      },
      "outputs": [],
      "source": [
        "# Creating a Series (1D) - like a single column\n",
        "ages = pd.Series([25, 30, 35, 40, 45], name='Age')\n",
        "print(\"📊 This is a Series (1D data):\")\n",
        "print(ages)\n",
        "print(f\"Type: {type(ages)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Creating a DataFrame (2D) - like a full spreadsheet\n",
        "simple_data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
        "    'Age': [25, 30, 35],\n",
        "    'City': ['New York', 'London', 'Tokyo']\n",
        "}\n",
        "simple_df = pd.DataFrame(simple_data)\n",
        "print(\"\\n📊 This is a DataFrame (2D data):\")\n",
        "print(simple_df)\n",
        "print(f\"Type: {type(simple_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f2b7b88",
      "metadata": {
        "id": "6f2b7b88"
      },
      "source": [
        "## 🔍 Section 3: Inspecting Your Data - The \"Holy Trinity\"\n",
        "\n",
        "When you get a new dataset, these are the first commands you should ALWAYS run. Think of them as your data detective tools! 🕵️‍♀️\n",
        "\n",
        "### The Holy Trinity of Data Inspection:\n",
        "1. `df.head()` - See the first few rows (like scrolling to the top in Excel)\n",
        "2. `df.info()` - Get the \"health report\" of your data\n",
        "3. `df.describe()` - Get statistical summary (like Excel's summary statistics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e43c93fc",
      "metadata": {
        "id": "e43c93fc"
      },
      "outputs": [],
      "source": [
        "# 1️⃣ df.head() - See the first few rows (default is 5)\n",
        "print(\"🔸 df.head() - First look at our data:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# You can also see the last few rows\n",
        "print(\"\\n🔸 df.tail() - Last few rows:\")\n",
        "print(df.tail(3))  # Show last 3 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c203f230",
      "metadata": {
        "id": "c203f230"
      },
      "outputs": [],
      "source": [
        "# 2️⃣ df.info() - The \"health report\" of your data (SUPER IMPORTANT!)\n",
        "print(\"🔸 df.info() - Health report of our dataset:\")\n",
        "print(df.info())\n",
        "print(\"\\n💡 This tells you:\")\n",
        "print(\"   - Column names and their data types\")\n",
        "print(\"   - How many non-null (not missing) values each column has\")\n",
        "print(\"   - Memory usage\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb95614",
      "metadata": {
        "id": "beb95614"
      },
      "outputs": [],
      "source": [
        "# 3️⃣ df.describe() - Statistical summary for numerical columns\n",
        "print(\"🔸 df.describe() - Statistical summary:\")\n",
        "print(df.describe())\n",
        "print(\"\\n💡 This gives you mean, std, min, max, quartiles for numerical columns\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Other useful inspection methods\n",
        "print(\"\\n🔸 df.shape - Dimensions of your data:\")\n",
        "print(f\"Shape: {df.shape} (rows, columns)\")\n",
        "\n",
        "print(\"\\n🔸 df.columns - Column names:\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "print(\"\\n🔸 df.dtypes - Data types of each column:\")\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7d34282",
      "metadata": {
        "id": "b7d34282"
      },
      "source": [
        "## 📋 Section 4: Selecting Columns and Basic Indexing\n",
        "\n",
        "Now let's learn how to pick specific columns from our DataFrame. This is like selecting columns in Excel!\n",
        "\n",
        "### 🎯 Key Concepts:\n",
        "- `df['column']` → Returns a **Series** (single column)\n",
        "- `df[['col1', 'col2']]` → Returns a **DataFrame** (multiple columns)\n",
        "- Notice the double brackets for multiple columns!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f993cbe6",
      "metadata": {
        "id": "f993cbe6"
      },
      "outputs": [],
      "source": [
        "# Selecting a SINGLE column - returns a Series\n",
        "names = df['Name']\n",
        "print(\"🔸 Single column selection - df['Name']:\")\n",
        "print(f\"Type: {type(names)}\")\n",
        "print(names.head(3))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Selecting MULTIPLE columns - returns a DataFrame\n",
        "# Notice the DOUBLE brackets!\n",
        "subset = df[['Name', 'Age', 'Sex']]\n",
        "print(\"\\n🔸 Multiple column selection - df[['Name', 'Age', 'Sex']]:\")\n",
        "print(f\"Type: {type(subset)}\")\n",
        "print(subset.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b080a4c",
      "metadata": {
        "id": "9b080a4c"
      },
      "source": [
        "## 🎯 Section 5: Selecting Rows with iloc and loc\n",
        "\n",
        "Now for the really powerful stuff! Let's learn how to select specific rows. This is like clicking on row numbers in Excel.\n",
        "\n",
        "### 🔧 Two Methods:\n",
        "- **`.iloc`** → Integer Location (position-based): 0, 1, 2, 3...\n",
        "- **`.loc`** → Label Location (label-based): uses the actual index labels\n",
        "\n",
        "### 🚨 Important Difference:\n",
        "- `.iloc[0:3]` → Gets rows 0, 1, 2 (excludes 3)\n",
        "- `.loc[0:3]` → Gets rows 0, 1, 2, 3 (includes 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dfeb9f4",
      "metadata": {
        "id": "3dfeb9f4"
      },
      "outputs": [],
      "source": [
        "# .iloc - Integer Location (position-based)\n",
        "print(\"🔸 .iloc examples (position-based):\")\n",
        "print(\"\\nFirst row (position 0):\")\n",
        "print(df.iloc[0])\n",
        "\n",
        "print(\"\\nFirst 3 rows (positions 0, 1, 2):\")\n",
        "print(df.iloc[0:3])\n",
        "\n",
        "print(\"\\nRows 2, 4, 6 (specific positions):\")\n",
        "print(df.iloc[[2, 4, 6]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3671642",
      "metadata": {
        "id": "f3671642"
      },
      "outputs": [],
      "source": [
        "# .loc - Label Location (uses index labels)\n",
        "print(\"🔸 .loc examples (label-based):\")\n",
        "print(\"\\nRow with index label 0:\")\n",
        "print(df.loc[0])\n",
        "\n",
        "print(\"\\nRows with index labels 0 through 3 (INCLUSIVE!):\")\n",
        "print(df.loc[0:3])  # Note: this INCLUDES row 3!\n",
        "\n",
        "# Advanced: selecting rows AND columns\n",
        "print(\"\\nRows 0-2, only Name and Age columns:\")\n",
        "print(df.loc[0:2, ['Name', 'Age']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f60307f",
      "metadata": {
        "id": "5f60307f"
      },
      "source": [
        "## 🎭 Section 6: Filtering Data with Conditions (The Magic!)\n",
        "\n",
        "This is where pandas becomes incredibly powerful! Think of this as Excel's AutoFilter, but much more flexible.\n",
        "\n",
        "### 🔥 The Process:\n",
        "1. Create a condition (returns True/False for each row)\n",
        "2. Use that condition to filter the DataFrame\n",
        "3. Combine multiple conditions with `&` (AND) and `|` (OR)\n",
        "\n",
        "### 🚨 Important: Always use `&` and `|`, NOT `and` and `or`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36de18d3",
      "metadata": {
        "id": "36de18d3"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create a condition (this returns True/False for each row)\n",
        "age_condition = df['Age'] > 30\n",
        "print(\"🔸 The condition df['Age'] > 30:\")\n",
        "print(age_condition)\n",
        "print(f\"\\nType: {type(age_condition)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Step 2: Use the condition to filter\n",
        "adults = df[df['Age'] > 30]\n",
        "print(\"\\n🔸 People older than 30:\")\n",
        "print(adults[['Name', 'Age']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd27025",
      "metadata": {
        "id": "2dd27025"
      },
      "outputs": [],
      "source": [
        "# More filtering examples\n",
        "print(\"🔸 Female passengers:\")\n",
        "females = df[df['Sex'] == 'female']\n",
        "print(females[['Name', 'Sex', 'Age']])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Combining conditions with & (AND) - MUST use parentheses!\n",
        "print(\"\\n🔸 Female passengers older than 25:\")\n",
        "young_females = df[(df['Sex'] == 'female') & (df['Age'] > 25)]\n",
        "print(young_females[['Name', 'Sex', 'Age']])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Using | (OR) - MUST use parentheses!\n",
        "print(\"\\n🔸 People in First Class OR who survived:\")\n",
        "first_or_survived = df[(df['Pclass'] == 1) | (df['Survived'] == 1)]\n",
        "print(first_or_survived[['Name', 'Pclass', 'Survived']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3b2ef43",
      "metadata": {
        "id": "e3b2ef43"
      },
      "source": [
        "## 🧰 Section 7: Essential Functions - unique() and value_counts()\n",
        "\n",
        "These two functions are absolute game-changers for exploring categorical data! They're like Excel's \"Remove Duplicates\" and \"Pivot Tables\" but much easier.\n",
        "\n",
        "### 🎯 When to use them:\n",
        "- **`.unique()`** → \"What are all the different values in this column?\"\n",
        "- **`.value_counts()`** → \"How many times does each value appear?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c7a21a8",
      "metadata": {
        "id": "6c7a21a8"
      },
      "outputs": [],
      "source": [
        "# .unique() - Get all unique values\n",
        "print(\"🔸 Unique values in 'Sex' column:\")\n",
        "print(df['Sex'].unique())\n",
        "\n",
        "print(\"\\n🔸 Unique values in 'Pclass' column:\")\n",
        "print(df['Pclass'].unique())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# .value_counts() - Count how many times each value appears\n",
        "print(\"\\n🔸 Count of each gender:\")\n",
        "print(df['Sex'].value_counts())\n",
        "\n",
        "print(\"\\n🔸 Count of each passenger class:\")\n",
        "print(df['Pclass'].value_counts())\n",
        "\n",
        "print(\"\\n🔸 Survival counts:\")\n",
        "print(df['Survived'].value_counts())\n",
        "\n",
        "# Pro tip: Add percentages!\n",
        "print(\"\\n🔸 Survival rates (as percentages):\")\n",
        "print(df['Survived'].value_counts(normalize=True) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7a7529b",
      "metadata": {
        "id": "b7a7529b"
      },
      "source": [
        "## 📊 Section 8: Sorting Data\n",
        "\n",
        "Sorting is like clicking the column headers in Excel to arrange your data. Super useful for finding patterns!\n",
        "\n",
        "### 🎯 Key Points:\n",
        "- `df.sort_values(by='column')` → Sort by one column\n",
        "- `ascending=False` → Sort from highest to lowest\n",
        "- Sort by multiple columns for more complex ordering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d0da23",
      "metadata": {
        "id": "25d0da23"
      },
      "outputs": [],
      "source": [
        "# Sort by Age (youngest to oldest)\n",
        "print(\"🔸 Sorted by Age (youngest first):\")\n",
        "sorted_by_age = df.sort_values(by='Age')\n",
        "print(sorted_by_age[['Name', 'Age']].head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Sort by Age (oldest to youngest)\n",
        "print(\"\\n🔸 Sorted by Age (oldest first):\")\n",
        "sorted_by_age_desc = df.sort_values(by='Age', ascending=False)\n",
        "print(sorted_by_age_desc[['Name', 'Age']].head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Sort by multiple columns\n",
        "print(\"\\n🔸 Sorted by Class (ascending), then Fare (descending):\")\n",
        "multi_sort = df.sort_values(by=['Pclass', 'Fare'], ascending=[True, False])\n",
        "print(multi_sort[['Name', 'Pclass', 'Fare']].head(8))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "177ee136",
      "metadata": {
        "id": "177ee136"
      },
      "source": [
        "## 🧹 Section 9: Handling Missing Data (NaN)\n",
        "\n",
        "Real-world data is messy! Missing values (NaN = \"Not a Number\") are everywhere. Let's learn to deal with them like a pro.\n",
        "\n",
        "### 🎯 The Strategy:\n",
        "1. **Find** missing data → `df.isnull().sum()`\n",
        "2. **Remove** it → `df.dropna()`  \n",
        "3. **Fill** it → `df.fillna()`\n",
        "\n",
        "### 💡 Excel Analogy:\n",
        "This is like finding empty cells and deciding whether to delete the rows or fill them with something sensible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "554ae086",
      "metadata": {
        "id": "554ae086"
      },
      "outputs": [],
      "source": [
        "# 1️⃣ Find missing data\n",
        "print(\"🔸 Missing values in each column:\")\n",
        "missing_data = df.isnull().sum()\n",
        "print(missing_data)\n",
        "\n",
        "print(\"\\n🔸 Rows with missing Age values:\")\n",
        "missing_age_rows = df[df['Age'].isnull()]\n",
        "print(missing_age_rows[['Name', 'Age']])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# 2️⃣ Remove missing data\n",
        "print(\"\\n🔸 Original shape:\", df.shape)\n",
        "df_no_missing = df.dropna()\n",
        "print(\"🔸 After removing missing values:\", df_no_missing.shape)\n",
        "print(\"🔸 We lost\", df.shape[0] - df_no_missing.shape[0], \"rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dddb948",
      "metadata": {
        "id": "6dddb948"
      },
      "outputs": [],
      "source": [
        "# 3️⃣ Fill missing data (better approach!)\n",
        "print(\"\\n🔸 Filling missing Age values:\")\n",
        "\n",
        "# Method 1: Fill with a specific value\n",
        "df_filled_zero = df.copy()\n",
        "df_filled_zero['Age'].fillna(0, inplace=True)\n",
        "print(\"Filled with 0:\", df_filled_zero['Age'].tolist())\n",
        "\n",
        "# Method 2: Fill with the mean (most common approach)\n",
        "df_filled_mean = df.copy()\n",
        "mean_age = df['Age'].mean()\n",
        "df_filled_mean['Age'].fillna(mean_age, inplace=True)\n",
        "print(f\"\\nFilled with mean ({mean_age:.1f}):\", df_filled_mean['Age'].tolist())\n",
        "\n",
        "# Method 3: Forward fill (use previous value)\n",
        "df_filled_ffill = df.copy()\n",
        "df_filled_ffill['Age'].fillna(method='ffill', inplace=True)\n",
        "print(\"\\nForward filled:\", df_filled_ffill['Age'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e12ecc17",
      "metadata": {
        "id": "e12ecc17"
      },
      "source": [
        "## 🛠️ Section 10: Creating and Modifying Columns\n",
        "\n",
        "One of pandas' superpowers is creating new columns from existing ones. This is like adding formulas in Excel, but much more powerful!\n",
        "\n",
        "### 🎯 Three Main Approaches:\n",
        "1. **Arithmetic operations** → Simple math between columns\n",
        "2. **`np.where()`** → If-then-else logic  \n",
        "3. **`.apply()`** → Apply custom functions to each row/value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882df1ff",
      "metadata": {
        "id": "882df1ff"
      },
      "outputs": [],
      "source": [
        "# Let's work with a clean dataset for this section\n",
        "df_clean = df.copy()\n",
        "df_clean['Age'].fillna(df_clean['Age'].mean(), inplace=True)  # Fill missing ages\n",
        "\n",
        "# 1️⃣ Arithmetic operations - Create new columns from existing ones\n",
        "print(\"🔸 Creating new columns with arithmetic:\")\n",
        "\n",
        "# Add some family size columns to our dataset first\n",
        "df_clean['SibSp'] = [1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0]  # Siblings/Spouses\n",
        "df_clean['Parch'] = [0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0]  # Parents/Children\n",
        "\n",
        "# Create family size (person + siblings/spouses + parents/children)\n",
        "df_clean['FamilySize'] = df_clean['SibSp'] + df_clean['Parch'] + 1\n",
        "print(\"Family Size calculation:\")\n",
        "print(df_clean[['Name', 'SibSp', 'Parch', 'FamilySize']].head())\n",
        "\n",
        "# Create fare per person\n",
        "df_clean['FarePerPerson'] = df_clean['Fare'] / df_clean['FamilySize']\n",
        "print(f\"\\nFare per person:\")\n",
        "print(df_clean[['Name', 'Fare', 'FamilySize', 'FarePerPerson']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7e63dec",
      "metadata": {
        "id": "f7e63dec"
      },
      "outputs": [],
      "source": [
        "# 2️⃣ Using np.where() for conditional logic (if-then-else)\n",
        "print(\"\\n🔸 Using np.where() for conditional columns:\")\n",
        "\n",
        "# Create age groups\n",
        "df_clean['AgeGroup'] = np.where(df_clean['Age'] < 18, 'Child',\n",
        "                               np.where(df_clean['Age'] < 65, 'Adult', 'Senior'))\n",
        "print(\"Age groups:\")\n",
        "print(df_clean[['Name', 'Age', 'AgeGroup']].head(8))\n",
        "\n",
        "# Create survival status in words\n",
        "df_clean['SurvivalStatus'] = np.where(df_clean['Survived'] == 1, 'Survived', 'Died')\n",
        "print(f\"\\nSurvival status:\")\n",
        "print(df_clean[['Name', 'Survived', 'SurvivalStatus']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6523075b",
      "metadata": {
        "id": "6523075b"
      },
      "outputs": [],
      "source": [
        "# 3️⃣ Using .apply() for more complex operations\n",
        "print(\"\\n🔸 Using .apply() for custom functions:\")\n",
        "\n",
        "# Extract name length\n",
        "df_clean['NameLength'] = df_clean['Name'].apply(len)\n",
        "print(\"Name lengths:\")\n",
        "print(df_clean[['Name', 'NameLength']].head())\n",
        "\n",
        "# Extract title from names (Mr., Mrs., Miss., etc.)\n",
        "def extract_title(name):\n",
        "    return name.split(',')[1].split('.')[0].strip()\n",
        "\n",
        "df_clean['Title'] = df_clean['Name'].apply(extract_title)\n",
        "print(f\"\\nTitles extracted:\")\n",
        "print(df_clean[['Name', 'Title']].head())\n",
        "\n",
        "# Custom function to categorize fare\n",
        "def fare_category(fare):\n",
        "    if fare < 10:\n",
        "        return 'Low'\n",
        "    elif fare < 30:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'High'\n",
        "\n",
        "df_clean['FareCategory'] = df_clean['Fare'].apply(fare_category)\n",
        "print(f\"\\nFare categories:\")\n",
        "print(df_clean[['Name', 'Fare', 'FareCategory']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8ad7969",
      "metadata": {
        "id": "f8ad7969"
      },
      "source": [
        "## 🏷️ Section 11: Dropping and Renaming Columns\n",
        "\n",
        "Sometimes you need to clean up your DataFrame by removing unnecessary columns or giving them better names. Think of this as cleaning up your Excel spreadsheet!\n",
        "\n",
        "### 🎯 Key Operations:\n",
        "- **Drop columns** → `df.drop('column', axis=1)`\n",
        "- **Rename columns** → `df.rename(columns={'old': 'new'})`\n",
        "- **Rename index** → `df.rename(index={0: 'first_row'})`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1297dcce",
      "metadata": {
        "id": "1297dcce"
      },
      "outputs": [],
      "source": [
        "# Current columns\n",
        "print(\"🔸 Current columns:\")\n",
        "print(list(df_clean.columns))\n",
        "\n",
        "# 1️⃣ Dropping columns\n",
        "print(\"\\n🔸 Dropping unnecessary columns:\")\n",
        "df_clean_dropped = df_clean.drop(['PassengerId', 'SibSp', 'Parch'], axis=1)\n",
        "print(\"After dropping PassengerId, SibSp, Parch:\")\n",
        "print(list(df_clean_dropped.columns))\n",
        "\n",
        "# You can also drop multiple columns at once\n",
        "# df_clean.drop(['col1', 'col2', 'col3'], axis=1)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# 2️⃣ Renaming columns for clarity\n",
        "print(\"\\n🔸 Renaming columns:\")\n",
        "df_renamed = df_clean_dropped.rename(columns={\n",
        "    'Pclass': 'PassengerClass',\n",
        "    'Sex': 'Gender',\n",
        "    'Age': 'AgeInYears',\n",
        "    'Fare': 'TicketPrice'\n",
        "})\n",
        "\n",
        "print(\"After renaming:\")\n",
        "print(list(df_renamed.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9b9b1e0",
      "metadata": {
        "id": "a9b9b1e0"
      },
      "outputs": [],
      "source": [
        "# 3️⃣ Renaming indexes\n",
        "print(\"\\n🔸 Renaming index:\")\n",
        "print(\"Original index:\", df_renamed.index.tolist())\n",
        "\n",
        "# Let's set passenger names as index and then rename some\n",
        "df_with_name_index = df_renamed.set_index('Name')\n",
        "print(\"\\nWith Name as index:\")\n",
        "print(df_with_name_index.head(3))\n",
        "\n",
        "# Rename specific index values\n",
        "df_with_name_index = df_with_name_index.rename(index={\n",
        "    'Braund, Mr. Owen Harris': 'Passenger_001',\n",
        "    'Cumings, Mrs. John Bradley': 'Passenger_002'\n",
        "})\n",
        "print(\"\\nAfter renaming some index values:\")\n",
        "print(df_with_name_index.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b1f26d",
      "metadata": {
        "id": "91b1f26d"
      },
      "source": [
        "## 🔧 Section 12: Changing Data Types\n",
        "\n",
        "Data types matter! Sometimes pandas guesses wrong, or you need to optimize memory usage. This is like formatting cells in Excel.\n",
        "\n",
        "### 🎯 Common Data Types:\n",
        "- `int64` → Whole numbers  \n",
        "- `float64` → Decimal numbers\n",
        "- `object` → Text/strings (and mixed types)\n",
        "- `category` → Limited set of values (saves memory!)\n",
        "- `bool` → True/False\n",
        "- `datetime64` → Dates and times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac3a1cee",
      "metadata": {
        "id": "ac3a1cee"
      },
      "outputs": [],
      "source": [
        "# Let's go back to our main dataset for this section\n",
        "print(\"🔸 Current data types:\")\n",
        "print(df_clean.dtypes)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# 1️⃣ Converting to categorical (saves memory for repeated values)\n",
        "print(\"\\n🔸 Converting to categorical:\")\n",
        "print(f\"Gender unique values: {df_clean['Sex'].unique()}\")\n",
        "df_clean['Sex'] = df_clean['Sex'].astype('category')\n",
        "print(f\"Sex data type after conversion: {df_clean['Sex'].dtype}\")\n",
        "\n",
        "# Convert passenger class to category too\n",
        "df_clean['Pclass'] = df_clean['Pclass'].astype('category')\n",
        "print(f\"Pclass data type: {df_clean['Pclass'].dtype}\")\n",
        "\n",
        "# 2️⃣ Converting numbers\n",
        "print(\"\\n🔸 Converting numeric types:\")\n",
        "# Convert Survived to boolean\n",
        "df_clean['Survived_Bool'] = df_clean['Survived'].astype(bool)\n",
        "print(f\"Survived as boolean: {df_clean['Survived_Bool'].dtype}\")\n",
        "print(f\"Values: {df_clean['Survived_Bool'].head().tolist()}\")\n",
        "\n",
        "# Convert fare to integer (removing decimals)\n",
        "df_clean['Fare_Int'] = df_clean['Fare'].astype(int)\n",
        "print(f\"Fare as integer: {df_clean['Fare_Int'].head().tolist()}\")\n",
        "print(f\"Original Fare: {df_clean['Fare'].head().tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4afe77de",
      "metadata": {
        "id": "4afe77de"
      },
      "source": [
        "## 🚀 Section 13: Groupby Operations - The Split-Apply-Combine Pattern\n",
        "\n",
        "This is THE most powerful feature in pandas! Think of it as Excel PivotTables but much more flexible.\n",
        "\n",
        "### 🎯 The Magic Pattern:\n",
        "1. **Split** → Divide data into groups based on column values\n",
        "2. **Apply** → Perform calculations on each group  \n",
        "3. **Combine** → Merge results back together\n",
        "\n",
        "### 📊 Common Aggregation Functions:\n",
        "- `.sum()`, `.count()`, `.mean()`, `.median()`\n",
        "- `.min()`, `.max()`, `.std()`, `.var()`\n",
        "- `.nunique()` → Count unique values\n",
        "- `.agg()` → Apply multiple functions at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "807db7f3",
      "metadata": {
        "id": "807db7f3"
      },
      "outputs": [],
      "source": [
        "# Let's understand the groupby process step by step\n",
        "print(\"🔸 Understanding GroupBy:\")\n",
        "\n",
        "# Step 1: The Split - Create groups (this doesn't show results yet!)\n",
        "grouped = df_clean.groupby('Sex')\n",
        "print(f\"Grouped object type: {type(grouped)}\")\n",
        "print(f\"Groups created: {list(grouped.groups.keys())}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Step 2 & 3: Apply & Combine - Now we see results!\n",
        "print(\"\\n🔸 Basic aggregations by Gender:\")\n",
        "\n",
        "print(\"Average age by gender:\")\n",
        "print(df_clean.groupby('Sex')['Age'].mean())\n",
        "\n",
        "print(\"\\nSurvival rate by gender:\")\n",
        "print(df_clean.groupby('Sex')['Survived'].mean())\n",
        "\n",
        "print(\"\\nPassenger count by gender:\")\n",
        "print(df_clean.groupby('Sex').size())  # size() counts rows per group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b569d00c",
      "metadata": {
        "id": "b569d00c"
      },
      "outputs": [],
      "source": [
        "# 🔥 The POWER of .agg() - Multiple aggregations at once!\n",
        "print(\"\\n🔸 Multiple aggregations with .agg():\")\n",
        "\n",
        "# Apply different functions to different columns\n",
        "result = df_clean.groupby('Sex').agg({\n",
        "    'Age': ['mean', 'min', 'max', 'std'],     # Multiple functions for Age\n",
        "    'Fare': ['mean', 'median', 'sum'],        # Multiple functions for Fare\n",
        "    'Survived': ['sum', 'mean', 'count']      # Multiple functions for Survived\n",
        "})\n",
        "\n",
        "print(result)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Group by Passenger Class\n",
        "print(\"\\n🔸 Analysis by Passenger Class:\")\n",
        "class_analysis = df_clean.groupby('Pclass').agg({\n",
        "    'Age': ['mean', 'std'],\n",
        "    'Fare': ['mean', 'min', 'max'],\n",
        "    'Survived': ['sum', 'mean'],\n",
        "    'FamilySize': ['mean', 'max']\n",
        "})\n",
        "print(class_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a99bc0d",
      "metadata": {
        "id": "1a99bc0d"
      },
      "outputs": [],
      "source": [
        "# 📊 More Statistical Functions\n",
        "print(\"\\n🔸 Statistical functions by group:\")\n",
        "\n",
        "print(\"Standard deviation of Age by Gender:\")\n",
        "print(df_clean.groupby('Sex')['Age'].std())\n",
        "\n",
        "print(\"\\nVariance of Fare by Class:\")\n",
        "print(df_clean.groupby('Pclass')['Fare'].var())\n",
        "\n",
        "print(\"\\nNumber of unique ages by Class:\")\n",
        "print(df_clean.groupby('Pclass')['Age'].nunique())\n",
        "\n",
        "print(\"\\nMinimum and Maximum fare by Class:\")\n",
        "print(\"Minimum:\", df_clean.groupby('Pclass')['Fare'].min().to_dict())\n",
        "print(\"Maximum:\", df_clean.groupby('Pclass')['Fare'].max().to_dict())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# All basic stats at once using describe()\n",
        "print(\"\\n🔸 Complete statistical summary by Gender:\")\n",
        "print(df_clean.groupby('Sex')['Age'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc08ec13",
      "metadata": {
        "id": "cc08ec13"
      },
      "source": [
        "## 🎯 Section 14: Advanced Grouping and Aggregations\n",
        "\n",
        "Let's take groupby to the next level! Multiple grouping columns, transformations, and filtering groups.\n",
        "\n",
        "### 🔧 Advanced Techniques:\n",
        "- **Multiple columns** → `df.groupby(['col1', 'col2'])`\n",
        "- **Transform** → Add group statistics back to original DataFrame\n",
        "- **Filter groups** → Keep only groups that meet certain criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "440f9080",
      "metadata": {
        "id": "440f9080"
      },
      "outputs": [],
      "source": [
        "# 1️⃣ Grouping by Multiple Columns\n",
        "print(\"🔸 Grouping by Class AND Gender:\")\n",
        "multi_group = df_clean.groupby(['Pclass', 'Sex']).agg({\n",
        "    'Age': 'mean',\n",
        "    'Fare': 'mean',\n",
        "    'Survived': ['count', 'sum', 'mean']\n",
        "})\n",
        "print(multi_group)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# 2️⃣ Transform - Add group statistics back to original DataFrame\n",
        "print(\"\\n🔸 Transform - Adding group averages back to each row:\")\n",
        "\n",
        "# Add the average age per gender to each row\n",
        "df_clean['AvgAgeByGender'] = df_clean.groupby('Sex')['Age'].transform('mean')\n",
        "\n",
        "# Add the average fare per class to each row\n",
        "df_clean['AvgFareByClass'] = df_clean.groupby('Pclass')['Fare'].transform('mean')\n",
        "\n",
        "# Show the results\n",
        "print(df_clean[['Name', 'Sex', 'Age', 'AvgAgeByGender', 'Pclass', 'Fare', 'AvgFareByClass']].head(8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6afd64fc",
      "metadata": {
        "id": "6afd64fc"
      },
      "outputs": [],
      "source": [
        "# 3️⃣ Filter Groups - Keep only groups that meet criteria\n",
        "print(\"\\n🔸 Filter Groups:\")\n",
        "\n",
        "# Keep only gender groups that have more than 5 people\n",
        "large_gender_groups = df_clean.groupby('Sex').filter(lambda x: len(x) > 5)\n",
        "print(f\"Original data: {len(df_clean)} rows\")\n",
        "print(f\"After filtering: {len(large_gender_groups)} rows\")\n",
        "print(\"Groups kept:\", large_gender_groups['Sex'].unique())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Create deviation from group mean\n",
        "print(\"\\n🔸 Calculating deviations from group mean:\")\n",
        "df_clean['AgeDeviation'] = df_clean.groupby('Sex')['Age'].transform(lambda x: x - x.mean())\n",
        "df_clean['FareDeviation'] = df_clean.groupby('Pclass')['Fare'].transform(lambda x: x - x.mean())\n",
        "\n",
        "print(\"Age and Fare deviations from group means:\")\n",
        "print(df_clean[['Name', 'Sex', 'Age', 'AgeDeviation', 'Pclass', 'Fare', 'FareDeviation']].head(8))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4caa1b94",
      "metadata": {
        "id": "4caa1b94"
      },
      "source": [
        "## 🔗 Section 15: Combining DataFrames with concat and merge\n",
        "\n",
        "Sometimes you have data spread across multiple files or tables. Pandas provides powerful tools to combine them!\n",
        "\n",
        "### 🎯 Two Main Approaches:\n",
        "- **`concat()`** → Stack DataFrames (like copying and pasting in Excel)\n",
        "- **`merge()`** → Join DataFrames on common columns (like VLOOKUP or SQL JOIN)\n",
        "\n",
        "### 📋 Merge Types:\n",
        "- `inner` → Keep only matching records\n",
        "- `left` → Keep all from left DataFrame  \n",
        "- `right` → Keep all from right DataFrame\n",
        "- `outer` → Keep all records from both"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40e47759",
      "metadata": {
        "id": "40e47759"
      },
      "outputs": [],
      "source": [
        "# Let's create some example DataFrames for demonstration\n",
        "df1 = pd.DataFrame({\n",
        "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
        "    'Age': [25, 30, 35],\n",
        "    'City': ['NYC', 'LA', 'Chicago']\n",
        "})\n",
        "\n",
        "df2 = pd.DataFrame({\n",
        "    'Name': ['David', 'Eve', 'Frank'],\n",
        "    'Age': [28, 32, 29],\n",
        "    'City': ['Boston', 'Seattle', 'Miami']\n",
        "})\n",
        "\n",
        "print(\"🔸 DataFrame 1:\")\n",
        "print(df1)\n",
        "print(\"\\n🔸 DataFrame 2:\")\n",
        "print(df2)\n",
        "\n",
        "# 1️⃣ Concatenation - Stacking DataFrames\n",
        "print(\"\\n🔸 Concatenating (stacking) DataFrames:\")\n",
        "combined = pd.concat([df1, df2])\n",
        "print(combined)\n",
        "print(f\"Notice the index: {combined.index.tolist()}\")\n",
        "\n",
        "# Reset index after concatenation\n",
        "combined_reset = pd.concat([df1, df2], ignore_index=True)\n",
        "print(f\"\\nAfter resetting index: {combined_reset.index.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dafe7b7",
      "metadata": {
        "id": "7dafe7b7"
      },
      "outputs": [],
      "source": [
        "# 2️⃣ Merging - Joining DataFrames on common columns\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\\n🔸 Merging DataFrames:\")\n",
        "\n",
        "# Create DataFrames with a common column to join on\n",
        "customers = pd.DataFrame({\n",
        "    'CustomerID': [1, 2, 3, 4],\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
        "    'Age': [25, 30, 35, 28]\n",
        "})\n",
        "\n",
        "orders = pd.DataFrame({\n",
        "    'OrderID': [101, 102, 103, 104, 105],\n",
        "    'CustomerID': [1, 2, 2, 3, 5],  # Note: CustomerID 5 doesn't exist in customers!\n",
        "    'Product': ['Laptop', 'Phone', 'Tablet', 'Camera', 'Watch'],\n",
        "    'Amount': [1000, 800, 300, 600, 200]\n",
        "})\n",
        "\n",
        "print(\"Customers DataFrame:\")\n",
        "print(customers)\n",
        "print(\"\\nOrders DataFrame:\")\n",
        "print(orders)\n",
        "\n",
        "# Inner merge (only matching records)\n",
        "print(\"\\n🔸 Inner Merge (only matching records):\")\n",
        "inner_merged = pd.merge(customers, orders, on='CustomerID', how='inner')\n",
        "print(inner_merged)\n",
        "\n",
        "# Left merge (all customers, even those without orders)\n",
        "print(\"\\n🔸 Left Merge (all customers):\")\n",
        "left_merged = pd.merge(customers, orders, on='CustomerID', how='left')\n",
        "print(left_merged)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28409069",
      "metadata": {
        "id": "28409069"
      },
      "source": [
        "## 🗃️ Section 16: Working with Index\n",
        "\n",
        "The index is like the \"row names\" in your DataFrame. Understanding it unlocks more advanced pandas operations!\n",
        "\n",
        "### 🎯 Key Operations:\n",
        "- **Set index** → `df.set_index('column')`\n",
        "- **Reset index** → `df.reset_index()`\n",
        "- **Multi-index** → Multiple levels of indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a873401a",
      "metadata": {
        "id": "a873401a"
      },
      "outputs": [],
      "source": [
        "# Working with our Titanic data\n",
        "print(\"🔸 Current index:\")\n",
        "print(f\"Index: {df_clean.index.tolist()}\")\n",
        "print(f\"Index name: {df_clean.index.name}\")\n",
        "\n",
        "# 1️⃣ Set a meaningful column as index\n",
        "df_with_name_index = df_clean.set_index('Name')\n",
        "print(\"\\n🔸 After setting 'Name' as index:\")\n",
        "print(df_with_name_index.head(3))\n",
        "print(f\"Index name: {df_with_name_index.index.name}\")\n",
        "\n",
        "# Now we can select rows by name!\n",
        "print(\"\\n🔸 Selecting by passenger name:\")\n",
        "alice_row = df_with_name_index.loc['Heikkinen, Miss. Laina']\n",
        "print(alice_row[['Age', 'Sex', 'Survived']])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# 2️⃣ Reset index back to default\n",
        "df_reset = df_with_name_index.reset_index()\n",
        "print(\"\\n🔸 After resetting index:\")\n",
        "print(df_reset.head(3))\n",
        "print(f\"Index: {df_reset.index.tolist()}\")\n",
        "print(\"Notice 'Name' is now a regular column again!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d67d20e",
      "metadata": {
        "id": "6d67d20e"
      },
      "source": [
        "## 💾 Section 17: Saving Results\n",
        "\n",
        "After all that hard work analyzing and cleaning data, you'll want to save your results!\n",
        "\n",
        "### 🎯 Common Export Formats:\n",
        "- **CSV** → `df.to_csv('filename.csv')` (most common)\n",
        "- **Excel** → `df.to_excel('filename.xlsx')`\n",
        "- **JSON** → `df.to_json('filename.json')`\n",
        "- **Pickle** → `df.to_pickle('filename.pkl')` (preserves all pandas data types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff2a81f3",
      "metadata": {
        "id": "ff2a81f3"
      },
      "outputs": [],
      "source": [
        "# Saving our cleaned and enhanced dataset\n",
        "print(\"🔸 Saving DataFrames:\")\n",
        "\n",
        "# Most common: Save to CSV\n",
        "# Note: index=False prevents saving the row numbers as a column\n",
        "df_clean.to_csv('titanic_cleaned.csv', index=False)\n",
        "print(\"✅ Saved to 'titanic_cleaned.csv'\")\n",
        "\n",
        "# Save only specific columns\n",
        "df_clean[['Name', 'Age', 'Sex', 'Survived', 'FamilySize']].to_csv('titanic_summary.csv', index=False)\n",
        "print(\"✅ Saved summary to 'titanic_summary.csv'\")\n",
        "\n",
        "# Save our grouped analysis\n",
        "survival_by_class = df_clean.groupby('Pclass')['Survived'].agg(['count', 'sum', 'mean'])\n",
        "survival_by_class.to_csv('survival_by_class.csv')\n",
        "print(\"✅ Saved analysis to 'survival_by_class.csv'\")\n",
        "\n",
        "print(f\"\\n📊 Final dataset shape: {df_clean.shape}\")\n",
        "print(f\"📋 Columns: {list(df_clean.columns)}\")\n",
        "\n",
        "# Show a sample of what we saved\n",
        "print(\"\\n🔸 Sample of cleaned data:\")\n",
        "print(df_clean[['Name', 'Age', 'Sex', 'Pclass', 'Survived', 'FamilySize', 'AgeGroup']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab1167f2",
      "metadata": {
        "id": "ab1167f2"
      },
      "source": [
        "## 🎯 Section 18: Mini-Project - Putting It All Together!\n",
        "\n",
        "Let's do a complete end-to-end analysis using everything we've learned! We'll answer some interesting questions about the Titanic passengers.\n",
        "\n",
        "### 🕵️‍♀️ Our Research Questions:\n",
        "1. What was the survival rate by passenger class and gender?\n",
        "2. Who were the youngest and oldest survivors?\n",
        "3. What was the average fare paid by survivors vs non-survivors?\n",
        "4. How did family size affect survival chances?\n",
        "5. Which passenger class had the highest survival rate?\n",
        "\n",
        "Let's investigate! 🔍"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4005e3",
      "metadata": {
        "id": "3a4005e3"
      },
      "outputs": [],
      "source": [
        "# 🔍 Question 1: Survival rate by passenger class and gender\n",
        "print(\"🔍 QUESTION 1: Survival rate by class and gender\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "survival_analysis = df_clean.groupby(['Pclass', 'Sex']).agg({\n",
        "    'Survived': ['count', 'sum', 'mean'],\n",
        "    'Age': 'mean',\n",
        "    'Fare': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "print(survival_analysis)\n",
        "\n",
        "print(\"\\n💡 Key Insights:\")\n",
        "print(\"- First class passengers had much higher survival rates\")\n",
        "print(\"- Women had significantly higher survival rates than men\")\n",
        "print(\"- First class female passengers had the highest survival rate\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# 🔍 Question 2: Youngest and oldest survivors\n",
        "print(\"\\n🔍 QUESTION 2: Youngest and oldest survivors\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "survivors = df_clean[df_clean['Survived'] == 1]\n",
        "\n",
        "youngest_survivor = survivors.loc[survivors['Age'].idxmin()]\n",
        "oldest_survivor = survivors.loc[survivors['Age'].idxmax()]\n",
        "\n",
        "print(f\"👶 Youngest survivor: {youngest_survivor['Name']}, Age: {youngest_survivor['Age']}\")\n",
        "print(f\"👴 Oldest survivor: {oldest_survivor['Name']}, Age: {oldest_survivor['Age']}\")\n",
        "\n",
        "print(f\"\\n📊 Age statistics for survivors:\")\n",
        "print(survivors['Age'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f935e52",
      "metadata": {
        "id": "7f935e52"
      },
      "outputs": [],
      "source": [
        "# 🔍 Question 3: Average fare by survival status\n",
        "print(\"\\n🔍 QUESTION 3: Average fare - Survivors vs Non-survivors\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "fare_by_survival = df_clean.groupby('Survived').agg({\n",
        "    'Fare': ['mean', 'median', 'std', 'min', 'max'],\n",
        "    'Age': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "print(fare_by_survival)\n",
        "print(f\"\\n💰 Survivors paid on average: ${df_clean[df_clean['Survived']==1]['Fare'].mean():.2f}\")\n",
        "print(f\"💸 Non-survivors paid on average: ${df_clean[df_clean['Survived']==0]['Fare'].mean():.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# 🔍 Question 4: Family size and survival\n",
        "print(\"\\n🔍 QUESTION 4: How family size affected survival\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "family_survival = df_clean.groupby('FamilySize').agg({\n",
        "    'Survived': ['count', 'sum', 'mean'],\n",
        "    'Age': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "print(family_survival)\n",
        "\n",
        "print(f\"\\n👨‍👩‍👧‍👦 Family size survival rates:\")\n",
        "for size in sorted(df_clean['FamilySize'].unique()):\n",
        "    rate = df_clean[df_clean['FamilySize']==size]['Survived'].mean()\n",
        "    count = len(df_clean[df_clean['FamilySize']==size])\n",
        "    print(f\"   Family size {size}: {rate:.1%} survival rate ({count} passengers)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e4e7af0",
      "metadata": {
        "id": "9e4e7af0"
      },
      "outputs": [],
      "source": [
        "# 🔍 Question 5: Which class had the highest survival rate?\n",
        "print(\"\\n🔍 QUESTION 5: Survival rate by passenger class\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class_survival = df_clean.groupby('Pclass').agg({\n",
        "    'Survived': ['count', 'sum', 'mean'],\n",
        "    'Fare': 'mean',\n",
        "    'Age': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "print(class_survival)\n",
        "\n",
        "print(f\"\\n🏆 FINAL SUMMARY:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for pclass in sorted(df_clean['Pclass'].unique()):\n",
        "    passengers = len(df_clean[df_clean['Pclass']==pclass])\n",
        "    survivors = df_clean[df_clean['Pclass']==pclass]['Survived'].sum()\n",
        "    rate = df_clean[df_clean['Pclass']==pclass]['Survived'].mean()\n",
        "    avg_fare = df_clean[df_clean['Pclass']==pclass]['Fare'].mean()\n",
        "\n",
        "    print(f\"📊 Class {pclass}: {survivors}/{passengers} survived ({rate:.1%}) - Avg fare: ${avg_fare:.2f}\")\n",
        "\n",
        "# Overall survival rate\n",
        "overall_rate = df_clean['Survived'].mean()\n",
        "total_passengers = len(df_clean)\n",
        "total_survivors = df_clean['Survived'].sum()\n",
        "\n",
        "print(f\"\\n🚢 OVERALL: {total_survivors}/{total_passengers} passengers survived ({overall_rate:.1%})\")\n",
        "\n",
        "print(f\"\\n🎯 TOP FINDINGS:\")\n",
        "print(\"   1. First-class passengers had the highest survival rate\")\n",
        "print(\"   2. Women were much more likely to survive than men\")\n",
        "print(\"   3. Passengers who paid higher fares were more likely to survive\")\n",
        "print(\"   4. Very large families had lower survival rates\")\n",
        "print(\"   5. Age played a role - children had better survival chances\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91ab8427",
      "metadata": {
        "id": "91ab8427"
      },
      "source": [
        "## 🎊 Congratulations! You've Mastered Pandas Basics!\n",
        "\n",
        "### 🏆 What You've Accomplished:\n",
        "✅ **Data Structures**: Mastered Series and DataFrames  \n",
        "✅ **Data Loading**: Read and inspect datasets like a pro  \n",
        "✅ **Data Selection**: Used `.loc`, `.iloc`, and boolean indexing  \n",
        "✅ **Data Filtering**: Created complex conditions with `&` and `|`  \n",
        "✅ **Data Exploration**: Used `.unique()`, `.value_counts()`, and more  \n",
        "✅ **Data Cleaning**: Handled missing values and data types  \n",
        "✅ **Feature Engineering**: Created new columns with arithmetic, `np.where()`, and `.apply()`  \n",
        "✅ **Data Organization**: Dropped, renamed, and reorganized columns  \n",
        "✅ **GroupBy Magic**: Performed powerful aggregations with min, max, mean, std, and more  \n",
        "✅ **Data Combination**: Merged and concatenated DataFrames  \n",
        "✅ **Index Management**: Set and reset indexes  \n",
        "✅ **Data Export**: Saved your results to CSV files  \n",
        "✅ **Real Analysis**: Completed a full end-to-end data analysis project!  \n",
        "\n",
        "### 🚀 Next Steps:\n",
        "- **Plotting**: Learn to create visualizations with `matplotlib` and `seaborn`\n",
        "- **Advanced Pandas**: Time series analysis, pivot tables, and performance optimization\n",
        "- **Machine Learning**: Use `scikit-learn` with your pandas skills\n",
        "- **Real Datasets**: Practice with Kaggle datasets and real-world data\n",
        "\n",
        "### 💪 You're Ready For Real-World Data Analysis!\n",
        "\n",
        "Remember: Pandas is like Excel, but with superpowers. You now have those superpowers! 🦸‍♀️\n",
        "\n",
        "---\n",
        "\n",
        "**Keep practicing and happy data analyzing!** 📊✨"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}